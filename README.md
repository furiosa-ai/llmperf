# LLMPerf

A Tool for evaulation the performance of LLM APIs.

# Installation
```bash
git clone https://github.com/ray-project/llmperf.git
cd llmperf
pip install -e .
```

# Basic Usage

We implement a load test for evaluating LLMs to check for performance. The test spawns a number of concurrent requests to the LLM API and measures the inter-token latency and generation throughput per request and across concurrent requests.

We support various llm clients, datasets, and scenarios. To run the most basic load test you can the `benchmark.py` script.


### Caveats and Disclaimers

- The endpoints provider backend might vary widely, so this is not a reflection on how the software runs on a particular hardware.
- The results may vary with time of day.
- The results may vary with the load.
- The results may not correlate with usersâ€™ workloads.

### FuriosaAI APIs
```bash
export FURIOSA_API_BASE=FURIOSA_API_ENDOPINT

python benchmark.py \
--model "meta-llama/Llama-3.2-1B-Instruct" \
--dataset translation \
--mean-input-tokens 550 \
--stddev-input-tokens 150 \
--mean-output-tokens 150 \
--stddev-output-tokens 10 \
--max-num-completed-requests 2 \
--timeout 600 \
--num-concurrent-requests 1 \
--wait-for any \
--results-dir "result_outputs" \
--llm-api furiosa \
--additional-sampling-params '{}'

```

### OpenAI Compatible APIs
```bash
export OPENAI_API_KEY=secret_abcdefg
export OPENAI_API_BASE="https://api.endpoints.anyscale.com/v1"

python benchmark.py \
--model "meta-llama/Llama-3.2-1B-Instruct" \
--dataset sonnet \
--mean-input-tokens 550 \
--stddev-input-tokens 150 \
--mean-output-tokens 150 \
--stddev-output-tokens 10 \
--max-num-completed-requests 2 \
--timeout 600 \
--num-concurrent-requests 1 \
--wait-for any \
--results-dir "result_outputs" \
--llm-api openai \
--additional-sampling-params '{}'

```

see `python benchmark.py --help` for more details on the arguments.


# Advanced Usage

```python
import ray
from transformers import LlamaTokenizerFast

from llmperf.ray_clients.openai_chat_completions_client import (
    OpenAIChatCompletionsClient,
)
from llmperf.launcher.wait_for_all import WaitForAllLauncher


# Copying the environment variables and passing them to ray.init() is necessary
# For making any clients work.
ray.init(runtime_env={"env_vars": {"OPENAI_API_BASE" : "https://api.endpoints.anyscale.com/v1",
                                   "OPENAI_API_KEY" : "YOUR_API_KEY"}})

MODEL="meta-llama/Llama-2-7b-chat-hf"

base_prompt = "hello_world"
tokenizer = LlamaTokenizerFast.from_pretrained(
    "hf-internal-testing/llama-tokenizer"
)
get_token_len = lambda text: len(tokenizer.encode(text))

base_prompt_len = get_token_len(base_prompt)
prompt = (base_prompt, base_prompt_len)

# Create a client for spawning requests
clients = [OpenAIChatCompletionsClient.remote(get_token_len)]

req_launcher = WaitForAllLauncher(MODEL, clients, {})

result = req_launcher.launch(0, [prompt], [128])
print(result)

```

# Implementing New LLM Clients

To implement a new LLM client, you need to implement the base class `llmperf.ray_clients.LLMClient` and decorate it as a ray actor.

```python

from llmperf.ray_clients import LLMClient
import ray


@ray.remote
class CustomLLMClient(LLMClient):

    def llm_request(self, request_config: RequestConfig) -> Tuple[Metrics, str, RequestConfig]:
        """Make a single completion request to a LLM API

        Returns:
            Metrics about the performance charateristics of the request.
            The text generated by the request to the LLM API.
            The request_config used to make the request. This is mainly for logging purposes.

        """
        ...

```

# Legacy Codebase
The old LLMPerf code base can be found in the [llmperf-legacy](https://github.com/ray-project/llmval-legacy) repo.
